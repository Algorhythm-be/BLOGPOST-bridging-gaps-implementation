{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridging gaps: The value of academic research for data scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implement it: Application to the 2022 Olympic Winter Games\n",
    "Now that we've seen the theoretical basis behind the ResNet, it's time for an application! While many machine learning models are implemented in well-known Python libraries, the ResNet for time series classification is not. No need to panic! We will not have to optimise our loss functions by hand, we can just rely on a deep learning framework, such as TensorFlow or PyTorch. To get a feel for the power of the ResNet, we will now try to predict whether a country has won a medal at the Olympic Winter Games in 2022 just by looking at their GDP from 1990 to 2018.\n",
    "\n",
    "Let's start by importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "\n",
    "import joblib\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "The data we use was collected from the web (https://olympics.com/en/olympic-games/beijing-2022/medals and https://www.kaggle.com/datasets/nitishabharathi/gdp-per-capita-all-countries) and put into files that can easily be read by Python. Let's start by opening these files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/GDP.csv\")\n",
    "data.rename(columns={\"Country \": \"Country\"}, inplace=True)\n",
    "data[\"Country\"] = data[\"Country\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "medals = open(\"datasets/medals.txt\").read().splitlines()[1:]\n",
    "participation = open(\"datasets/participation.txt\").read().splitlines()[1:]\n",
    "countries = [i.strip() for i in open(\"datasets/countries.txt\").read().splitlines()[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every country participated in the Olympic Winter Games of 2022. Let's therefore make a variable that indicates whether that country participated and one that indicates whether they won a medal. We only include countries that participated. In addition, the ResNet requires full time series without missing values. For the purpose of this example, we choose to delete 18 cases with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Country</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>...</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>medal</th>\n",
       "      <th>participated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>2549.473022</td>\n",
       "      <td>1909.114038</td>\n",
       "      <td>1823.307673</td>\n",
       "      <td>2057.449657</td>\n",
       "      <td>2289.873135</td>\n",
       "      <td>2665.764906</td>\n",
       "      <td>2980.066288</td>\n",
       "      <td>...</td>\n",
       "      <td>10207.752350</td>\n",
       "      <td>10526.235450</td>\n",
       "      <td>10571.010650</td>\n",
       "      <td>11259.225890</td>\n",
       "      <td>11662.030480</td>\n",
       "      <td>11868.178970</td>\n",
       "      <td>12930.140030</td>\n",
       "      <td>13364.15540</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>ARG</td>\n",
       "      <td>7380.115031</td>\n",
       "      <td>8210.643432</td>\n",
       "      <td>8942.569853</td>\n",
       "      <td>9777.214005</td>\n",
       "      <td>10435.910770</td>\n",
       "      <td>10225.118710</td>\n",
       "      <td>10857.429670</td>\n",
       "      <td>...</td>\n",
       "      <td>19817.450480</td>\n",
       "      <td>19764.225010</td>\n",
       "      <td>20365.613350</td>\n",
       "      <td>20008.320640</td>\n",
       "      <td>20551.833190</td>\n",
       "      <td>20130.408030</td>\n",
       "      <td>20843.155070</td>\n",
       "      <td>20610.56855</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>ARM</td>\n",
       "      <td>2428.558960</td>\n",
       "      <td>2237.752728</td>\n",
       "      <td>1356.210786</td>\n",
       "      <td>1296.178498</td>\n",
       "      <td>1429.102386</td>\n",
       "      <td>1591.894846</td>\n",
       "      <td>1742.734114</td>\n",
       "      <td>...</td>\n",
       "      <td>7019.767748</td>\n",
       "      <td>7649.061531</td>\n",
       "      <td>8003.087763</td>\n",
       "      <td>8405.073655</td>\n",
       "      <td>8727.385447</td>\n",
       "      <td>8808.572714</td>\n",
       "      <td>9620.818491</td>\n",
       "      <td>10343.17559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "      <td>17329.706610</td>\n",
       "      <td>17790.980140</td>\n",
       "      <td>18189.378740</td>\n",
       "      <td>19131.841870</td>\n",
       "      <td>20064.459010</td>\n",
       "      <td>20894.397210</td>\n",
       "      <td>21972.052650</td>\n",
       "      <td>...</td>\n",
       "      <td>41965.358420</td>\n",
       "      <td>42826.789580</td>\n",
       "      <td>45902.047950</td>\n",
       "      <td>46880.220660</td>\n",
       "      <td>46276.150690</td>\n",
       "      <td>47305.880020</td>\n",
       "      <td>49628.811810</td>\n",
       "      <td>51663.36509</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>Austria</td>\n",
       "      <td>AUT</td>\n",
       "      <td>19442.312040</td>\n",
       "      <td>20585.019310</td>\n",
       "      <td>21259.636750</td>\n",
       "      <td>21698.307670</td>\n",
       "      <td>22606.829640</td>\n",
       "      <td>23660.408610</td>\n",
       "      <td>24529.207830</td>\n",
       "      <td>...</td>\n",
       "      <td>44452.732750</td>\n",
       "      <td>46457.345780</td>\n",
       "      <td>47922.049120</td>\n",
       "      <td>48799.715470</td>\n",
       "      <td>49879.266470</td>\n",
       "      <td>51809.513630</td>\n",
       "      <td>53937.066380</td>\n",
       "      <td>55454.68929</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    Country Country Code          1990          1991          1992  \\\n",
       "1      3    Albania          ALB   2549.473022   1909.114038   1823.307673   \n",
       "4      6  Argentina          ARG   7380.115031   8210.643432   8942.569853   \n",
       "5      7    Armenia          ARM   2428.558960   2237.752728   1356.210786   \n",
       "7      9  Australia          AUS  17329.706610  17790.980140  18189.378740   \n",
       "8     10    Austria          AUT  19442.312040  20585.019310  21259.636750   \n",
       "\n",
       "           1993          1994          1995          1996  ...          2011  \\\n",
       "1   2057.449657   2289.873135   2665.764906   2980.066288  ...  10207.752350   \n",
       "4   9777.214005  10435.910770  10225.118710  10857.429670  ...  19817.450480   \n",
       "5   1296.178498   1429.102386   1591.894846   1742.734114  ...   7019.767748   \n",
       "7  19131.841870  20064.459010  20894.397210  21972.052650  ...  41965.358420   \n",
       "8  21698.307670  22606.829640  23660.408610  24529.207830  ...  44452.732750   \n",
       "\n",
       "           2012          2013          2014          2015          2016  \\\n",
       "1  10526.235450  10571.010650  11259.225890  11662.030480  11868.178970   \n",
       "4  19764.225010  20365.613350  20008.320640  20551.833190  20130.408030   \n",
       "5   7649.061531   8003.087763   8405.073655   8727.385447   8808.572714   \n",
       "7  42826.789580  45902.047950  46880.220660  46276.150690  47305.880020   \n",
       "8  46457.345780  47922.049120  48799.715470  49879.266470  51809.513630   \n",
       "\n",
       "           2017         2018  medal  participated  \n",
       "1  12930.140030  13364.15540      0             1  \n",
       "4  20843.155070  20610.56855      0             1  \n",
       "5   9620.818491  10343.17559      0             1  \n",
       "7  49628.811810  51663.36509      1             1  \n",
       "8  53937.066380  55454.68929      1             1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"medal\"] = np.where(data[\"Country\"].isin(medals), 1, 0)\n",
    "data[\"participated\"] = np.where(data[\"Country\"].isin(participation), 1, 0)\n",
    "data.drop(columns='2019',inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(inplace=True)\n",
    "data = data[data[\"participated\"]==1]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data, consisting of 71 countries, now needs to be split into test and training data. Since we want to predict medal winners, the variable 'medal' becomes our target. GDP from 1990 to 2018 represents the features used for this prediction. Since most countries do not win a medal, there is some class imbalance. We therefore stratify our splits to make sure both our training and test sample contain the same percentage of medal winning countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"medal\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "x = data.loc[:, \"1990\":\"2018\"].to_numpy()\n",
    "x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "We are now ready to build and train the model. Before we set out the structure of the ResNet, we first set some parameters, such as the number of epochs and the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature_maps = 64 #number of filters\n",
    "input_shape = x_train.shape[1:] \n",
    "nb_classes = 1 #there is only one final node \n",
    "nb_epochs = 1500 #1500 epochs\n",
    "mini_batch_size = 10 #a batch size of 10\n",
    "output_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "#BLOCK 1\n",
    "#layer 1\n",
    "conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "#layer 2\n",
    "conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "#layer 3\n",
    "conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "#add raw input to output of layer 3\n",
    "shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "#BLOCK 2\n",
    "#layer 1\n",
    "conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "#layer 2\n",
    "conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "#layer 3\n",
    "conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "#add output of block 1 to output of layer 3\n",
    "shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
    "output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "#BLOCK 3\n",
    "#layer 1\n",
    "conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "#layer 2\n",
    "conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "#layer 3\n",
    "conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "#add output of block 2 to output of layer 3\n",
    "shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
    "\n",
    "output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
    "output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "\n",
    "#FINAL\n",
    "#global average pooling\n",
    "gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
    "\n",
    "#final dense layer\n",
    "output_layer = keras.layers.Dense(nb_classes, activation='sigmoid')(gap_layer)\n",
    "\n",
    "#specify and compile model\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
    "              metrics=['binary_accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model is compiled, we are ready to train it. Before we do so, we introduce some callbacks. More specifically, we save the best model in terms of training AUC at the end of each epoch. In addition, we allow for early stopping if the training loss does not decline any further. Afterwards, we train our model on the data. Because of the substantial randomness involved in setting the original weights, we set seeds to allow reproduction of our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = output_directory + '\\\\best_model.h5py'\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='auc',\n",
    "                                                    save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=0.00001, patience=50, mode=\"min\")\n",
    "\n",
    "callbacks = [model_checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(98566)\n",
    "tf.random.set_seed(451246)\n",
    "hist = model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
    "                verbose=0, validation_data=(x_test, y_test), callbacks=callbacks)\n",
    "\n",
    "model.save(output_directory + '\\\\last_model.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the results\n",
    "However good the model may be, without some sense-making it remains a black box. Let's first start by inspecting its performance. Throughout the epochs, the model should become better at matching the data. The figure shows that both the training and test loss decline througout the epochs. After about 350 epochs, our training stops since training loss has not decreased for several epochs. We also plot the test loss, yet do not make decisions based on this quantity. As expected, the test loss is always slightly higher than the training loss. Both training and test accuracy rapidly rise after the first 50 epochs and then remain almost constant at 80-85%. The AUC follows a similar pattern and remains constant at around 0.9. Overall, the model performs well. Since the accuracy of a classification problem is highly dependent on the chosen threshold, we choose our best model based on the training AUC. This is maximal around 210 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(hist.history)\n",
    "joblib.dump(hist_df, \"history.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range = np.arange(1, len(hist_df)+1)\n",
    "\n",
    "plt.style.use('bmh')\n",
    "fig, ax = plt.subplots(3, 1, sharex=\"col\", figsize=(5,13))\n",
    "ax[0].plot(range, hist_df[\"loss\"], label='Test Loss', color=\"steelblue\")\n",
    "ax[0].plot(range, hist_df[\"val_loss\"], label='Training Loss', color=\"darkslateblue\")\n",
    "ax[0].vlines(hist_df.loss.idxmin(axis=0), 0, 1, color=\"crimson\")\n",
    "ax[0].text(hist_df.loss.idxmin(axis=0) - 10, 0.05, f\"Min Train Loss ({round(min(hist_df.loss), 2)})\", rotation=\"vertical\")\n",
    "ax[0].vlines(hist_df.val_loss.idxmin(axis=0), 0, 1, color=\"crimson\")\n",
    "ax[0].text(hist_df.val_loss.idxmin(axis=0) - 10, 0.05, f\"Min Test Loss ({round(min(hist_df.val_loss), 2)})\", rotation=\"vertical\")\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "\n",
    "ax[1].plot(range, hist_df[\"binary_accuracy\"], label='Test Accuracy', color=\"steelblue\")\n",
    "ax[1].plot(range, hist_df[\"val_binary_accuracy\"], label='Training Accuracy', color=\"darkslateblue\")\n",
    "ax[1].vlines(hist_df.binary_accuracy.idxmax(axis=0), 0, 1, color=\"crimson\")\n",
    "ax[1].text(hist_df.binary_accuracy.idxmax(axis=0) - 10, 0.05, f\"Max Train Accuracy ({round(max(hist_df.binary_accuracy), 2)})\", rotation=\"vertical\")\n",
    "ax[1].vlines(hist_df.val_binary_accuracy.idxmax(axis=0), 0, 1, color=\"crimson\")\n",
    "ax[1].text(hist_df.val_binary_accuracy.idxmax(axis=0) - 10, 0.05, f\"Max Test Accuracy ({round(max(hist_df.val_binary_accuracy), 2)})\", rotation=\"vertical\")\n",
    "ax[1].set_ylim(0,1)\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax[2].plot(range, hist_df[\"auc\"], label='Test', color=\"steelblue\")\n",
    "ax[2].plot(range, hist_df[\"val_auc\"], label='Training', color=\"darkslateblue\")\n",
    "ax[2].vlines(hist_df.auc.idxmax(axis=0), 0, 1, color=\"crimson\")\n",
    "ax[2].text(hist_df.auc.idxmax(axis=0) - 10, 0.05, f\"Max Train AUC ({round(max(hist_df.auc), 2)})\", rotation=\"vertical\")\n",
    "ax[2].vlines(hist_df.val_auc.idxmax(axis=0), 0, 1, color=\"crimson\")\n",
    "ax[2].text(hist_df.val_auc.idxmax(axis=0) - 10, 0.05, f\"Max Test AUC ({round(max(hist_df.val_auc), 2)})\", rotation=\"vertical\")\n",
    "ax[2].set_ylim(0,1)\n",
    "ax[2].legend()\n",
    "ax[2].set_ylabel(\"AUC\")\n",
    "ax[2].set_xlabel(\"Epoch\")\n",
    "\n",
    "fig.savefig(\"results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "<img src=\"figures/results.png\"/>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "Performance of the model in terms of loss, accuracy and AUC for both the test and training set\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(\"best_model.h5py\")\n",
    "y_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best model, we subsequently plot the ROC curve. With an AUC of 0.91 it performs significantly better than a random model. We choose the best threshold as the point on the curve that is closest to the upper left corner in terms of Euclidean distance, where the true positive rate is 1 and the false positive rate is 0. This point is indicated with a red dot. At this point, if we classify a country as a medal winner only if its predicted probability is above 0.54, we are able to achieve a false positive rate of 0.09 and a true positive rate of 0.76."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [np.sqrt(i**2 + (1-j)**2) for i, j in zip(fpr, tpr)]\n",
    "idx = distances.index(min(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('bmh')\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.plot([0, 1], [0, 1], 'k--', color=\"crimson\")\n",
    "ax.plot(fpr, tpr, color=\"steelblue\")\n",
    "ax.scatter(fpr[idx], tpr[idx], s=500, color=\"crimson\", marker=\"o\")\n",
    "ax.text(fpr[idx] - 0.1, tpr[idx] + 0.1, f\"({round(fpr[idx], 2)}, {round(tpr[idx], 2)})\")\n",
    "ax.text(0.6, 0.2, f\"AUC = {round(auc, 2)}\")\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.set_ylim(0,1.05)\n",
    "ax.set_xlim(-0.05,1)\n",
    "\n",
    "fig.savefig(\"roc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "<img src=\"figures/roc.png\"/>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "ROC curve for the model with the best training AUC\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5440258"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present an unbiased estimate of our model's performance, we calculate various statistics on the test set. Our model achieves an overall precision of about 85% and a recall of about 87%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No medal       0.92      0.86      0.89        14\n",
      "       Medal       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.86        22\n",
      "   macro avg       0.85      0.87      0.86        22\n",
      "weighted avg       0.87      0.86      0.87        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.where(model.predict(x_test) > thresholds[idx], 1, 0)\n",
    "print(classification_report(y_test, y_test_pred, target_names=[\"No medal\", \"Medal\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Activation Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model performs well in terms in terms of almost all metrics presented above. Yet, as with many deep learning models, it remains a black box. To find out what features of the time series add to its prediction, we can use a Class Activation Map. Before its final dense layer, the ResNet contains a Global Average Pooling (GAP) layer. Therefore, for a given time series with length $T$, let $S_k(t)$ represent the output (activation) of filter $k$ in the last convolutional layer for time point $t$. For this filter $k$, the output of the GAP layer is $f_k = \\sum_{t=1}^T S_k(t)$. If we let $w_k$ represent the weight of this output in the final dense layer, the input for the final softmax function is \n",
    "\n",
    "$g = \\sum_{k=1}^K w_k \\sum_{t=1}^T S_k(t) = \\sum_{k=1}^K \\sum_{t=1}^T w_k S_k(t)$\n",
    "\n",
    "The importance of each temporal element $t$ for the classification to class 1 is therefore  $\\sum_{k=1}^K w_k S_k(t)$. For the purposes of the Class Activation Map, we can ignore the softmax activation in this case, since it is a monotone transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = best_model.input\n",
    "output_before_GAP = best_model.layers[35].output\n",
    "new_model = keras.models.Model(input, output_before_GAP)\n",
    "tmp_pred = new_model.predict(x_test) #predict the S_k(t)\n",
    "\n",
    "weights = best_model.layers[37].get_weights()[0].reshape((1, 1, 128)) #get the weights\n",
    "cam = np.multiply(tmp_pred, weights) #multiply each S_k(t) by corresponding w_k\n",
    "cam = np.sum(cam, axis=2) #sum over all k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape((22, 29))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "inxval = np.array(data.loc[:, \"1990\":\"2018\"].columns.astype('int32'))\n",
    "\n",
    "for i in np.arange(0, x_test.shape[0]): \n",
    "    value = x_test[i, :]\n",
    "    color = cam[i, :]\n",
    "\n",
    "    points = np.array([inxval, value]).T.reshape(-1,1,2)\n",
    "    segments = np.concatenate([points[:-1],points[1:]], axis=1)\n",
    "\n",
    "    lc = LineCollection(segments, cmap=\"coolwarm\", linewidth=3)\n",
    "    # set color to date values\n",
    "    lc.set_array(color)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('GDP')\n",
    "\n",
    "ax.autoscale_view()\n",
    "\n",
    "fig.savefig(\"cam.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "<img src=\"figures/cam.png\"/>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "Class Activation Map for the test set (high values are indicated by a deep red, low values by a deep blue)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our class activation map shows the importance of each time point for the classification of that time series. It shows that especially the periods of high growth contribute to a higher probability of winning a medal at the 2022 Winter Olympics. Intuitively this makes sense: Wealthier countries have more funds to invest in the development of athletic capabilities and are therefore more likely to win a medal. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab3ff65c3a6eb1790baef8684e28a91227692a093f50cd4a6fbc81a7cfdc7c57"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('deeplearning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
